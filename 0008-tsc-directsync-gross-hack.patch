diff -rupN zen-kernel-5.13.9-zen1.orig/arch/x86/kernel/tsc.c zen-kernel-5.13.9-zen1/arch/x86/kernel/tsc.c
--- zen-kernel-5.13.9-zen1.orig/arch/x86/kernel/tsc.c	2021-08-08 22:35:53.320588666 +0200
+++ zen-kernel-5.13.9-zen1/arch/x86/kernel/tsc.c	2021-08-08 22:38:56.842824944 +0200
@@ -47,6 +47,7 @@ static unsigned int __initdata tsc_early
 static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
 int tsc_clocksource_reliable;
+int tsc_allow_direct_sync;
 
 static u32 art_to_tsc_numerator;
 static u32 art_to_tsc_denominator;
@@ -303,6 +304,10 @@ static int __init tsc_setup(char *str)
 		mark_tsc_unstable("boot parameter");
 	if (!strcmp(str, "nowatchdog"))
 		no_tsc_watchdog = 1;
+	if (!strcmp(str, "directsync")) {
+		tsc_allow_direct_sync = 1;
+		no_tsc_watchdog = 1;
+	}
 	return 1;
 }
 
diff -rupN zen-kernel-5.13.9-zen1.orig/arch/x86/kernel/tsc_sync.c zen-kernel-5.13.9-zen1/arch/x86/kernel/tsc_sync.c
--- zen-kernel-5.13.9-zen1.orig/arch/x86/kernel/tsc_sync.c	2021-08-08 22:35:53.320588666 +0200
+++ zen-kernel-5.13.9-zen1/arch/x86/kernel/tsc_sync.c	2021-08-08 22:47:32.655946254 +0200
@@ -31,6 +31,8 @@ struct tsc_adjust {
 
 static DEFINE_PER_CPU(struct tsc_adjust, tsc_adjust);
 
+extern int tsc_allow_direct_sync;
+
 /*
  * TSC's on different sockets may be reset asynchronously.
  * This may cause the TSC ADJUST value on socket 0 to be NOT 0.
@@ -299,6 +301,8 @@ static cycles_t check_tsc_warp(unsigned
  */
 static inline unsigned int loop_timeout(int cpu)
 {
+	if (!boot_cpu_has(X86_FEATURE_TSC_ADJUST))
+		return 30;
 	return (cpumask_weight(topology_core_cpumask(cpu)) > 1) ? 2 : 20;
 }
 
@@ -322,10 +326,12 @@ void check_tsc_sync_source(int cpu)
 	 *  1 if the CPU does not provide the TSC_ADJUST MSR
 	 *  3 if the MSR is available, so the target can try to adjust
 	 */
-	if (!boot_cpu_has(X86_FEATURE_TSC_ADJUST))
-		atomic_set(&test_runs, 1);
-	else
+	if (boot_cpu_has(X86_FEATURE_TSC_ADJUST))
 		atomic_set(&test_runs, 3);
+	else if (tsc_allow_direct_sync)
+		atomic_set(&test_runs, 5);
+	else
+		atomic_set(&test_runs, 1);
 retry:
 	/*
 	 * Wait for the target to start or to skip the test:
@@ -365,11 +371,19 @@ retry:
 
 		pr_warn("TSC synchronization [CPU#%d -> CPU#%d]:\n",
 			smp_processor_id(), cpu);
-		pr_warn("Measured %Ld cycles TSC warp between CPUs, "
-			"turning off TSC clock.\n", max_warp);
 		if (random_warps)
 			pr_warn("TSC warped randomly between CPUs\n");
-		mark_tsc_unstable("check_tsc_sync_source failed");
+		if (max_warp > 1000) {
+			mark_tsc_unstable("check_tsc_sync_source failed");
+
+			pr_warn("Measured %Ld cycles TSC warp between CPUs, "
+				"turning off TSC clock.\n", max_warp);
+		} else {
+			pr_warn("Measured %Ld cycles TSC warp between CPUs, "
+				"leaving TSC on (hack, warp less that 1000 cycles)\n", max_warp);
+	
+		}
+
 	}
 
 	/*
@@ -393,6 +407,21 @@ retry:
 		goto retry;
 }
 
+static inline cycles_t write_tsc_adjustment(s64 adjustment)
+{
+	cycles_t adjval, nextval;
+
+	rdmsrl(MSR_IA32_TSC, adjval);
+	adjval += adjustment;
+	wrmsrl(MSR_IA32_TSC, adjval);
+	rdmsrl(MSR_IA32_TSC, nextval);
+
+	/*
+	 * Estimated clock cycle overhead for wrmsr + rdmsr
+	 */
+	return nextval - adjval;
+}
+
 /*
  * Freshly booted CPUs call into this:
  */
@@ -400,7 +429,7 @@ void check_tsc_sync_target(void)
 {
 	struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
 	unsigned int cpu = smp_processor_id();
-	cycles_t cur_max_warp, gbl_max_warp;
+	cycles_t cur_max_warp, gbl_max_warp, est_overhead = 0;
 	int cpus = 2;
 
 	/* Also aborts if there is no TSC. */
@@ -480,12 +509,18 @@ retry:
 	 * value is used. In the worst case the adjustment needs to go
 	 * through a 3rd run for fine tuning.
 	 */
-	cur->adjusted += cur_max_warp;
-
-	pr_warn("TSC ADJUST compensate: CPU%u observed %lld warp. Adjust: %lld\n",
-		cpu, cur_max_warp, cur->adjusted);
-
-	wrmsrl(MSR_IA32_TSC_ADJUST, cur->adjusted);
+	if (boot_cpu_has(X86_FEATURE_TSC_ADJUST)) {
+		cur->adjusted += cur_max_warp + est_overhead;
+		
+		pr_warn("TSC ADJUST compensate: CPU%u observed %lld warp. Adjust: %lld\n",
+			cpu, cur_max_warp, cur->adjusted);
+
+		wrmsrl(MSR_IA32_TSC_ADJUST, cur->adjusted);
+	} else {
+		pr_warn("TSC direct sync: CPU%u observed %lld warp. Overhead: %lld\n",
+			cpu, cur_max_warp, est_overhead);
+		est_overhead = write_tsc_adjustment(cur_max_warp + est_overhead);
+	}
 	goto retry;
 
 }
diff -rupN zen-kernel-5.13.9-zen1.orig/Documentation/admin-guide/kernel-parameters.txt zen-kernel-5.13.9-zen1/Documentation/admin-guide/kernel-parameters.txt
--- zen-kernel-5.13.9-zen1.orig/Documentation/admin-guide/kernel-parameters.txt	2021-08-08 22:35:52.780590125 +0200
+++ zen-kernel-5.13.9-zen1/Documentation/admin-guide/kernel-parameters.txt	2021-08-08 22:37:27.660089565 +0200
@@ -5688,6 +5688,8 @@
 			in situations with strict latency requirements (where
 			interruptions from clocksource watchdog are not
 			acceptable).
+			[x86] directsync: attempt to sync the tsc via direct
+			writes if MSR_IA32_TSC_ADJUST isn't available
 
 	tsc_early_khz=  [X86] Skip early TSC calibration and use the given
 			value instead. Useful when the early TSC frequency discovery
